{
  "paragraphs": [
    {
      "text": "%md\n### Files\nhadoop /breweries/breweries.csv \\\nhadoop /ml_data (movies) \\\nhadoop /dumpert",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:29:40.867",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "markdown",
          "editOnDblClick": true
        },
        "editorMode": "ace/mode/markdown",
        "colWidth": 12.0,
        "editorHide": true,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "HTML",
            "data": "\u003cdiv class\u003d\"markdown-body\"\u003e\n\u003ch3\u003eFiles\u003c/h3\u003e\n\u003cp\u003ehadoop /breweries/breweries.csv \u003cbr /\u003e\nhadoop /ml_data (movies) \u003cbr /\u003e\nhadoop /dumpert\u003c/p\u003e\n\n\u003c/div\u003e"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896448_1496000344",
      "id": "20210610-222456_2074716056",
      "dateCreated": "2021-06-10 22:24:56.448",
      "dateStarted": "2021-06-10 22:29:40.869",
      "dateFinished": "2021-06-10 22:29:40.875",
      "status": "FINISHED"
    },
    {
      "text": "!pip install pyspark\n# %pyspark\n\n# Start spark session\nfrom pyspark.sql import SparkSession\n\nspark \u003d SparkSession \\\n        .builder \\\n        .appName(\"Streaming\") \\\n        .master(\"spark://spark-master:7077\") \\\n        .getOrCreate()",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:29:40.969",
      "progress": 0,
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionSupport": true,
          "completionKey": "TAB"
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "editorHide": false,
        "fontSize": 9.0,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.zeppelin.interpreter.InterpreterException: java.io.IOException: Interpreter process is not running\n/opt/zeppelin/bin/interpreter.sh: line 294: /zeppelin/spark/bin/spark-submit: No such file or directory\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:129)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:271)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:444)\n\tat org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:72)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:182)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Interpreter process is not running\n/opt/zeppelin/bin/interpreter.sh: line 294: /zeppelin/spark/bin/spark-submit: No such file or directory\n\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:157)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:126)\n\t... 13 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896449_518378041",
      "id": "20210610-222456_145439421",
      "dateCreated": "2021-06-10 22:24:56.449",
      "dateStarted": "2021-06-10 22:29:40.972",
      "dateFinished": "2021-06-10 22:29:40.972",
      "status": "ERROR"
    },
    {
      "text": "%python\n# Check if files are interpreted correctly\n\nfrom pyspark.sql.functions import *\n\npath \u003d \"hdfs://namenode:9000/dumpert/videos/9999\"\njson \u003d spark.read.json(path)\nvideos \u003d json.select(explode(\"items\").alias(\"videos\"))\nvideos \u003d videos.select( \\\n      col(\"videos.date\").alias(\"date\") \\\n    , col(\"videos.description\").alias(\"description\") \\\n    , col(\"videos.id\").alias(\"id\") \\\n    , col(\"videos.media\").alias(\"media\") \\\n    , col(\"videos.nopreroll\").alias(\"nopreroll\") \\\n    , col(\"videos.nsfw\").alias(\"nsfw\") \\\n    , col(\"videos.stats\").alias(\"stats\") \\\n    , col(\"videos.still\").alias(\"still\") \\\n    , col(\"videos.stills\").alias(\"stills\") \\\n    , col(\"videos.tags\").alias(\"tags\") \\\n    , col(\"videos.thumbnail\").alias(\"thumbnail\") \\\n    , col(\"videos.title\").alias(\"title\") \n    )\n\nvideos.show(1, vertical\u003dTrue)\n# parentDF.printSchema()",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.449",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "-RECORD 0---------------------------\n date        | 2008-03-01 07:12:41  \n description | \"Conjonedak or so... \n id          | 43365_47842255       \n media       | [[, 209, VIDEO, [... \n nopreroll   | false                \n nsfw        | false                \n stats       | [0, 3905, 0, 87743]  \n still       | https://media.dum... \n stills      | [https://media.du... \n tags        | amerikaan europa ... \n thumbnail   | https://media.dum... \n title       | We staan weer op ... \nonly showing top 1 row\n\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896449_12917291",
      "id": "20210610-222456_686877754",
      "dateCreated": "2021-06-10 22:24:56.449",
      "status": "READY"
    },
    {
      "text": "%python\nfrom pyspark.sql.types import *\n\ncountsDF \u003d videos.groupBy(videos.nopreroll).count()\ncountsDF.cache()\n\n# Register the DataFrame as table \u0027static_counts\u0027\ncountsDF.createOrReplaceTempView(\"static_counts\")",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.449",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896449_14137918",
      "id": "20210610-222456_1616815754",
      "dateCreated": "2021-06-10 22:24:56.449",
      "status": "READY"
    },
    {
      "text": "%python\n# !pip install ipython-sql\n%load_ext sql\n%sql select action, sum(count) as total_count from static_counts group by action",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.449",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "Requirement already satisfied: ipython-sql in /usr/local/lib/python3.7/dist-packages (0.4.0)\nRequirement already satisfied: ipython\u003e\u003d1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (7.24.1)\nRequirement already satisfied: ipython-genutils\u003e\u003d0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.2.0)\nRequirement already satisfied: prettytable\u003c1 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.7.2)\nRequirement already satisfied: six in /usr/lib/python3/dist-packages (from ipython-sql) (1.12.0)\nRequirement already satisfied: sqlalchemy\u003e\u003d0.6.7 in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (1.4.18)\nRequirement already satisfied: sqlparse in /usr/local/lib/python3.7/dist-packages (from ipython-sql) (0.4.1)\nRequirement already satisfied: jedi\u003e\u003d0.16 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (0.18.0)\nRequirement already satisfied: traitlets\u003e\u003d4.2 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (5.0.5)\nRequirement already satisfied: setuptools\u003e\u003d18.5 in /usr/lib/python3/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (40.8.0)\nRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (0.2.0)\nRequirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (5.0.9)\nRequirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (0.1.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (0.7.5)\nRequirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (2.9.0)\nRequirement already satisfied: pexpect\u003e4.3 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (4.8.0)\nRequirement already satisfied: prompt-toolkit!\u003d3.0.0,!\u003d3.0.1,\u003c3.1.0,\u003e\u003d2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython\u003e\u003d1.0-\u003eipython-sql) (3.0.18)\nRequirement already satisfied: parso\u003c0.9.0,\u003e\u003d0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi\u003e\u003d0.16-\u003eipython\u003e\u003d1.0-\u003eipython-sql) (0.8.2)\nRequirement already satisfied: ptyprocess\u003e\u003d0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect\u003e4.3-\u003eipython\u003e\u003d1.0-\u003eipython-sql) (0.7.0)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!\u003d3.0.0,!\u003d3.0.1,\u003c3.1.0,\u003e\u003d2.0.0-\u003eipython\u003e\u003d1.0-\u003eipython-sql) (0.2.5)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy\u003e\u003d0.6.7-\u003eipython-sql) (4.5.0)\nRequirement already satisfied: greenlet!\u003d0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy\u003e\u003d0.6.7-\u003eipython-sql) (1.1.0)\nRequirement already satisfied: typing-extensions\u003e\u003d3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003esqlalchemy\u003e\u003d0.6.7-\u003eipython-sql) (3.10.0.0)\nRequirement already satisfied: zipp\u003e\u003d0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-\u003esqlalchemy\u003e\u003d0.6.7-\u003eipython-sql) (3.4.1)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nEnvironment variable $DATABASE_URL not set, and no connect string given.\nConnection info needed in SQLAlchemy format, example:\n               postgresql://username:password@hostname/dbname\n               or an existing connection: dict_keys([])\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896449_1170513582",
      "id": "20210610-222456_1331697543",
      "dateCreated": "2021-06-10 22:24:56.449",
      "status": "READY"
    },
    {
      "text": "%python\nfrom pyspark.sql.types import *\n\ninputPath \u003d \"hdfs://namenode:9000/dumpert/videos\"\n\n\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.450",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896450_367415838",
      "id": "20210610-222456_642437089",
      "dateCreated": "2021-06-10 22:24:56.450",
      "status": "READY"
    },
    {
      "text": "\nimport spark.implicits._            \n\n    \nfileStream \u003d spark.readStream \\\n                .format(\"\")\n    \n val file \u003d spark.readStream.schema(schemaforfile).csv(\"C:\\\\SparkScala\\\\fakefriends.csv\")  \n\n file.writeStream.format(\"parquet\").start(\"C:\\\\Users\\\\roswal01\\\\Desktop\\\\streamed\") \n \n spark.stop()\n\nval file \u003d spark.readStream.schema(schemaforfile).csv(\"C:\\\\SparkScala\\\\fakefriends.csv\")  \n\nval query \u003d file.writeStream.format(\"parquet\")\n    .option(\"checkpointLocation\", \"path/to/HDFS/dir\")\n    .start(\"C:\\\\Users\\\\roswal01\\\\Desktop\\\\streamed\") \n\nquery.awaitTermination()\n\ndf \u003d spark.readStream \\\n    .format(\"rate\") \\\n    .option(\"rowsPerSecond\", 1) \\\n    .load()\n\n# Write the streaming DataFrame to a table\ndf.writeStream \\\n    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n    .toTable(\"myTable\")\n\n# Check the table result\nspark.read.table(\"myTable\").show()\n\n# Transform the source dataset and write to a new table\nspark.readStream \\\n    .table(\"myTable\") \\\n    .select(\"value\") \\\n    .writeStream \\\n    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n    .format(\"parquet\") \\\n    .toTable(\"newTable\")\n\n# Check the new table result\nspark.read.table(\"newTable\").show()",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.450",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896450_1566721889",
      "id": "20210610-222456_535038119",
      "dateCreated": "2021-06-10 22:24:56.450",
      "status": "READY"
    },
    {
      "text": "%python\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.450",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896450_2111867594",
      "id": "20210610-222456_177964839",
      "dateCreated": "2021-06-10 22:24:56.450",
      "status": "READY"
    },
    {
      "text": "%python\n",
      "user": "anonymous",
      "dateUpdated": "2021-06-10 22:24:56.450",
      "progress": 0,
      "config": {
        "editorMode": "ace/mode/python",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1623363896450_343643637",
      "id": "20210610-222456_1158178268",
      "dateCreated": "2021-06-10 22:24:56.450",
      "status": "READY"
    }
  ],
  "name": "File stream",
  "id": "2G813WJMT",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false
  },
  "info": {
    "isRunning": true
  }
}