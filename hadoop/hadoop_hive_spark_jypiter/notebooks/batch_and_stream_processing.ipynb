{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d309f304",
   "metadata": {},
   "source": [
    "## Files\n",
    "hadoop /breweries/breweries.csv \\\n",
    "hadoop /ml_data (movies) \\\n",
    "hadoop /dumpert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73852ccf",
   "metadata": {},
   "source": [
    "## Start session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0192e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:03.856022\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import datetime\n",
    "\n",
    "# Start spark session\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Streaming\") \\\n",
    "        .config(\"spark.driver.memory\", \"15g\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555314a",
   "metadata": {},
   "source": [
    "## Single file processing (test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5186e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------------\n",
      " date        | 2008-03-01 07:12:41  \n",
      " description | \"Conjonedak or so... \n",
      " id          | 43365_47842255       \n",
      " media       | [[, 209, VIDEO, [... \n",
      " nopreroll   | false                \n",
      " nsfw        | false                \n",
      " stats       | [0, 3905, 0, 87743]  \n",
      " still       | https://media.dum... \n",
      " stills      | [https://media.du... \n",
      " tags        | amerikaan europa ... \n",
      " thumbnail   | https://media.dum... \n",
      " title       | We staan weer op ... \n",
      "only showing top 1 row\n",
      "\n",
      "0:00:06.281004\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# Check if files are interpreted correctly\n",
    "\n",
    "staticVideosTest = spark.read \\\n",
    "    .json(\"hdfs://namenode:9000/dumpert/videos/9999\") \\\n",
    "    .select(explode(\"items\").alias(\"videos\")) \\\n",
    "    .select( \\\n",
    "      col(\"videos.date\").alias(\"date\") \\\n",
    "    , col(\"videos.description\").alias(\"description\") \\\n",
    "    , col(\"videos.id\").alias(\"id\") \\\n",
    "    , col(\"videos.media\").alias(\"media\") \\\n",
    "    , col(\"videos.nopreroll\").alias(\"nopreroll\") \\\n",
    "    , col(\"videos.nsfw\").alias(\"nsfw\") \\\n",
    "    , col(\"videos.stats\").alias(\"stats\") \\\n",
    "    , col(\"videos.still\").alias(\"still\") \\\n",
    "    , col(\"videos.stills\").alias(\"stills\") \\\n",
    "    , col(\"videos.tags\").alias(\"tags\") \\\n",
    "    , col(\"videos.thumbnail\").alias(\"thumbnail\") \\\n",
    "    , col(\"videos.title\").alias(\"title\") \n",
    "    )\n",
    "\n",
    "staticVideosTest.show(1, vertical=True)\n",
    "# parentDF.printSchema()\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aad6ab83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.349677\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# 1. Tel het totaal aantal videos\n",
    "\n",
    "display(staticVideosTest.count())\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73c3a0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|count(id)|nopreroll|\n",
      "+---------+---------+\n",
      "|       15|    false|\n",
      "+---------+---------+\n",
      "\n",
      "0:00:02.285046\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# Register a table\n",
    "staticVideosTest.cache()\n",
    "staticVideosTest.createOrReplaceTempView(\"videos\")\n",
    "table = spark.sql(\"select count(id), nopreroll from videos group by nopreroll\")\n",
    "table.show()\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0177a3",
   "metadata": {},
   "source": [
    "## Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e77b52e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading videos took: \n",
      "0:00:40.153398\n",
      "Loading comments took: \n",
      "0:04:57.216998\n",
      "Caching took: \n",
      "0:00:00.076309\n",
      "-RECORD 0-----------------------------------------\n",
      " approved                  | true                 \n",
      " article_id                | 7109669              \n",
      " article_link              | https://comments.... \n",
      " article_title             | Even geduld nog aub  \n",
      " author_is_newbie          | false                \n",
      " author_username           | Unloadable           \n",
      " banned                    | false                \n",
      " child_comments            | []                   \n",
      " creation_datetime         | 2017-04-20 19:52:45  \n",
      " display_content           | K*t Ziggo...         \n",
      " html_markup               | <div class=\"cmt-c... \n",
      " id                        | 239927506            \n",
      " is_author_premium_visible | false                \n",
      " kudos_count               | -3                   \n",
      " parent_id                 | 0                    \n",
      " reference_id              | 0                    \n",
      " report_count              | 0                    \n",
      "only showing top 1 row\n",
      "\n",
      "-RECORD 0---------------------------\n",
      " date        | 2013-10-15 12:26:28  \n",
      " description | De NS denkt nu ec... \n",
      " id          | 6566890_a21801a4     \n",
      " media       | [[,, FOTO, [[http... \n",
      " nopreroll   | false                \n",
      " nsfw        | false                \n",
      " stats       | [0, 964, 0, 97493]   \n",
      " still       | https://media.dum... \n",
      " stills      | [https://media.du... \n",
      " tags        | trein ns spoor bl... \n",
      " thumbnail   | https://media.dum... \n",
      " title       | Bladblazers          \n",
      "only showing top 1 row\n",
      "\n",
      "Showing data took: \n",
      "0:00:02.557646\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load videos and comments\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "staticVideos = spark.read \\\n",
    "    .json(\"hdfs://namenode:9000/dumpert/videos/\") \\\n",
    "    .select(explode(\"items\").alias(\"videos\")) \\\n",
    "    .select( \\\n",
    "      col(\"videos.date\").alias(\"date\") \\\n",
    "    , col(\"videos.description\").alias(\"description\") \\\n",
    "    , col(\"videos.id\").alias(\"id\") \\\n",
    "    , col(\"videos.media\").alias(\"media\") \\\n",
    "    , col(\"videos.nopreroll\").alias(\"nopreroll\") \\\n",
    "    , col(\"videos.nsfw\").alias(\"nsfw\") \\\n",
    "    , col(\"videos.stats\").alias(\"stats\") \\\n",
    "    , col(\"videos.still\").alias(\"still\") \\\n",
    "    , col(\"videos.stills\").alias(\"stills\") \\\n",
    "    , col(\"videos.tags\").alias(\"tags\") \\\n",
    "    , col(\"videos.thumbnail\").alias(\"thumbnail\") \\\n",
    "    , col(\"videos.title\").alias(\"title\") \n",
    "    )\n",
    "print(\"Loading videos took: \") \n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "staticComments = spark.read \\\n",
    "     .json(\"hdfs://namenode:9000/dumpert/reaguursels/\") \\\n",
    "     .select(explode(\"data.comments\").alias(\"comments\")) \\\n",
    "     .select( \\\n",
    "      col(\"comments.approved\").alias(\"approved\") \\\n",
    "    , col(\"comments.article_id\").alias(\"article_id\") \\\n",
    "    , col(\"comments.article_link\").alias(\"article_link\") \\\n",
    "    , col(\"comments.article_title\").alias(\"article_title\") \\\n",
    "    , col(\"comments.author_is_newbie\").alias(\"author_is_newbie\") \\\n",
    "    , col(\"comments.author_username\").alias(\"author_username\") \\\n",
    "    , col(\"comments.banned\").alias(\"banned\") \\\n",
    "    , col(\"comments.child_comments\").alias(\"child_comments\") \\\n",
    "    , col(\"comments.creation_datetime\").alias(\"creation_datetime\") \\\n",
    "    , col(\"comments.display_content\").alias(\"display_content\") \\\n",
    "    , col(\"comments.html_markup\").alias(\"html_markup\") \\\n",
    "    , col(\"comments.id\").alias(\"id\") \\\n",
    "    , col(\"comments.is_author_premium_visible\").alias(\"is_author_premium_visible\") \\\n",
    "    , col(\"comments.kudos_count\").alias(\"kudos_count\") \\\n",
    "    , col(\"comments.parent_id\").alias(\"parent_id\") \\\n",
    "    , col(\"comments.reference_id\").alias(\"reference_id\") \\\n",
    "    , col(\"comments.report_count\").alias(\"report_count\")\n",
    "    )\n",
    "print(\"Loading comments took: \") \n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "# reaguurselsParentDF.printSchema()\n",
    "# videosParentDF.printSchema()\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "staticComments.cache()\n",
    "staticVideos.cache()\n",
    "print(\"Caching took: \")\n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "staticComments.show(1, vertical=True)\n",
    "staticVideos.show(1, vertical=True)\n",
    "print(\"Showing data took: \")\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f004a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164354"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7110782"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:20.930826\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.datetime.now()\n",
    "\n",
    "# 1. Tel het totaal aantal videos\n",
    "\n",
    "display(staticVideos.count())\n",
    "display(staticComments.count())\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59276805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|count(id)|nopreroll|\n",
      "+---------+---------+\n",
      "|    28568|     true|\n",
      "|   135786|    false|\n",
      "+---------+---------+\n",
      "\n",
      "0:00:03.342594\n",
      "+-----+---------+\n",
      "|kudos|count(id)|\n",
      "+-----+---------+\n",
      "| > 20|  1006972|\n",
      "+-----+---------+\n",
      "\n",
      "+-----+---------+\n",
      "|kudos|count(id)|\n",
      "+-----+---------+\n",
      "|< -20|   202281|\n",
      "+-----+---------+\n",
      "\n",
      "0:00:28.307170\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "\n",
    "# Videos\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "staticVideos.createOrReplaceTempView(\"videos\")\n",
    "table = spark.sql(\"SELECT COUNT(id), nopreroll FROM videos GROUP BY nopreroll\")\n",
    "table.show()\n",
    "\n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "# Comments\n",
    "startTime = datetime.datetime.now()\n",
    "\n",
    "staticComments.createOrReplaceTempView(\"comments\")\n",
    "table = spark.sql(\"SELECT '> 20' as kudos, COUNT(id) FROM comments WHERE kudos_count > 20\")\n",
    "table.show()\n",
    "table = spark.sql(\"SELECT '< -20' as kudos, COUNT(id) FROM comments WHERE kudos_count < -20\")\n",
    "table.show()\n",
    "\n",
    "print(datetime.datetime.now()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60102274",
   "metadata": {},
   "source": [
    "## Structured Stream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f9b6049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve '`data.comments`' given input columns: [approved, article_id, article_link, article_title, author_is_newbie, author_username, banned, child_comments, creation_datetime, display_content, html_markup, id, is_author_premium_visible, kudos_count, parent_id, reference_id, report_count];;\n'Project [explode('data.comments) AS comments#2463]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39964bc3,json,List(),Some(StructType(StructField(approved,BooleanType,true), StructField(article_id,LongType,true), StructField(article_link,StringType,true), StructField(article_title,StringType,true), StructField(author_is_newbie,BooleanType,true), StructField(author_username,StringType,true), StructField(banned,BooleanType,true), StructField(child_comments,ArrayType(StructType(StructField(approved,BooleanType,true), StructField(article_id,LongType,true), StructField(article_link,StringType,true), StructField(article_title,StringType,true), StructField(author_is_newbie,BooleanType,true), StructField(author_username,StringType,true), StructField(banned,BooleanType,true), StructField(creation_datetime,TimestampType,true), StructField(display_content,StringType,true), StructField(html_markup,StringType,true), StructField(id,LongType,true), StructField(is_author_premium_visible,BooleanType,true), StructField(kudos_count,LongType,true), StructField(parent_id,LongType,true), StructField(reference_id,LongType,true), StructField(report_count,LongType,true)),true),true), StructField(creation_datetime,TimestampType,true), StructField(display_content,StringType,true), StructField(html_markup,StringType,true), StructField(id,LongType,true), StructField(is_author_premium_visible,BooleanType,true), StructField(kudos_count,LongType,true), StructField(parent_id,LongType,true), StructField(reference_id,LongType,true), StructField(report_count,LongType,true))),List(),None,Map(maxFilesPerTrigger -> 1, path -> hdfs://namenode:9000/dumpert/reaguursels/),None), FileSource[hdfs://namenode:9000/dumpert/reaguursels/], [approved#2429, article_id#2430L, article_link#2431, article_title#2432, author_is_newbie#2433, author_username#2434, banned#2435, child_comments#2436, creation_datetime#2437, display_content#2438, html_markup#2439, id#2440L, is_author_premium_visible#2441, kudos_count#2442L, parent_id#2443L, reference_id#2444L, report_count#2445L]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f80a5eaf039e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maxFilesPerTrigger\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hdfs://namenode:9000/dumpert/reaguursels/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data.comments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comments\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     .select( \\\n\u001b[1;32m     11\u001b[0m       \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"comments.approved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"approved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \"\"\"\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jcols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve '`data.comments`' given input columns: [approved, article_id, article_link, article_title, author_is_newbie, author_username, banned, child_comments, creation_datetime, display_content, html_markup, id, is_author_premium_visible, kudos_count, parent_id, reference_id, report_count];;\n'Project [explode('data.comments) AS comments#2463]\n+- StreamingRelation DataSource(org.apache.spark.sql.SparkSession@39964bc3,json,List(),Some(StructType(StructField(approved,BooleanType,true), StructField(article_id,LongType,true), StructField(article_link,StringType,true), StructField(article_title,StringType,true), StructField(author_is_newbie,BooleanType,true), StructField(author_username,StringType,true), StructField(banned,BooleanType,true), StructField(child_comments,ArrayType(StructType(StructField(approved,BooleanType,true), StructField(article_id,LongType,true), StructField(article_link,StringType,true), StructField(article_title,StringType,true), StructField(author_is_newbie,BooleanType,true), StructField(author_username,StringType,true), StructField(banned,BooleanType,true), StructField(creation_datetime,TimestampType,true), StructField(display_content,StringType,true), StructField(html_markup,StringType,true), StructField(id,LongType,true), StructField(is_author_premium_visible,BooleanType,true), StructField(kudos_count,LongType,true), StructField(parent_id,LongType,true), StructField(reference_id,LongType,true), StructField(report_count,LongType,true)),true),true), StructField(creation_datetime,TimestampType,true), StructField(display_content,StringType,true), StructField(html_markup,StringType,true), StructField(id,LongType,true), StructField(is_author_premium_visible,BooleanType,true), StructField(kudos_count,LongType,true), StructField(parent_id,LongType,true), StructField(reference_id,LongType,true), StructField(report_count,LongType,true))),List(),None,Map(maxFilesPerTrigger -> 1, path -> hdfs://namenode:9000/dumpert/reaguursels/),None), FileSource[hdfs://namenode:9000/dumpert/reaguursels/], [approved#2429, article_id#2430L, article_link#2431, article_title#2432, author_is_newbie#2433, author_username#2434, banned#2435, child_comments#2436, creation_datetime#2437, display_content#2438, html_markup#2439, id#2440L, is_author_premium_visible#2441, kudos_count#2442L, parent_id#2443L, reference_id#2444L, report_count#2445L]\n"
     ]
    }
   ],
   "source": [
    "# Load comments\n",
    "## Treat a sequence of files as a stream by picking one file at a time\n",
    "\n",
    "startTime = datetime.datetime.now()\n",
    "streamingComments = spark.readStream \\\n",
    "    .schema(staticComments.schema) \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .json(\"hdfs://namenode:9000/dumpert/reaguursels/\") \\\n",
    "    .select(explode(\"data.comments\").alias(\"comments\")) \\\n",
    "    .select( \\\n",
    "      col(\"comments.approved\").alias(\"approved\") \\\n",
    "    , col(\"comments.article_id\").alias(\"article_id\") \\\n",
    "    , col(\"comments.article_link\").alias(\"article_link\") \\\n",
    "    , col(\"comments.article_title\").alias(\"article_title\") \\\n",
    "    , col(\"comments.author_is_newbie\").alias(\"author_is_newbie\") \\\n",
    "    , col(\"comments.author_username\").alias(\"author_username\") \\\n",
    "    , col(\"comments.banned\").alias(\"banned\") \\\n",
    "    , col(\"comments.child_comments\").alias(\"child_comments\") \\\n",
    "    , col(\"comments.creation_datetime\").alias(\"creation_datetime\") \\\n",
    "    , col(\"comments.display_content\").alias(\"display_content\") \\\n",
    "    , col(\"comments.html_markup\").alias(\"html_markup\") \\\n",
    "    , col(\"comments.id\").alias(\"id\") \\\n",
    "    , col(\"comments.is_author_premium_visible\").alias(\"is_author_premium_visible\") \\\n",
    "    , col(\"comments.kudos_count\").alias(\"kudos_count\") \\\n",
    "    , col(\"comments.parent_id\").alias(\"parent_id\") \\\n",
    "    , col(\"comments.reference_id\").alias(\"reference_id\") \\\n",
    "    , col(\"comments.report_count\").alias(\"report_count\")\n",
    "    )\n",
    "print(\"Loading comments took: \") \n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "\n",
    "# Start counting\n",
    "startTime = datetime.datetime.now()\n",
    "streamingCounts = comments.count()\n",
    "print(datetime.datetime.now()-startTime)\n",
    "\n",
    "streamingCounts.isStreaming\n",
    "\n",
    "\n",
    "#\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", \"2\")  # keep the size of shuffles small\n",
    "\n",
    "# query = streamingCountsDF \\\n",
    "#     .writeStream \\\n",
    "#     .format(\"memory\") \\       # memory = store in-memory table \n",
    "#     .queryName(\"counts\") \\    # counts = name of the in-memory table\n",
    "#     .outputMode(\"complete\") \\ # complete = all the counts should be in the table\n",
    "#     .start()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88ebc7bd",
   "metadata": {},
   "source": [
    "import spark.implicits._            \n",
    "\n",
    "    \n",
    "fileStream = spark.readStream \\\n",
    "                .format(\"\")\n",
    "    \n",
    " val file = spark.readStream.schema(schemaforfile).csv(\"C:\\\\SparkScala\\\\fakefriends.csv\")  \n",
    "\n",
    " file.writeStream.format(\"parquet\").start(\"C:\\\\Users\\\\roswal01\\\\Desktop\\\\streamed\") \n",
    " \n",
    " spark.stop()\n",
    "\n",
    "val file = spark.readStream.schema(schemaforfile).csv(\"C:\\\\SparkScala\\\\fakefriends.csv\")  \n",
    "\n",
    "val query = file.writeStream.format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\")\n",
    "    .start(\"C:\\\\Users\\\\roswal01\\\\Desktop\\\\streamed\") \n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load()\n",
    "\n",
    "# Write the streaming DataFrame to a table\n",
    "df.writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .toTable(\"myTable\")\n",
    "\n",
    "# Check the table result\n",
    "spark.read.table(\"myTable\").show()\n",
    "\n",
    "# Transform the source dataset and write to a new table\n",
    "spark.readStream \\\n",
    "    .table(\"myTable\") \\\n",
    "    .select(\"value\") \\\n",
    "    .writeStream \\\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .toTable(\"newTable\")\n",
    "\n",
    "# Check the new table result\n",
    "spark.read.table(\"newTable\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f790e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fddb47b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
