{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e2bd8ae-00c6-49fc-92aa-b02ff3796a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To start, this will create the SparkSession and set the environment to use our local MongoDB cluster.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"pyspark-notebook2\") \\\n",
    "        .master(\"spark://spark-master:7077\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .config(\"spark.mongodb.input.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/Stocks.Source?replicaSet=rs0\") \\\n",
    "        .config(\"spark.mongodb.output.uri\",\"mongodb://mongo1:27017,mongo2:27018,mongo3:27019/Stocks.Source?replicaSet=rs0\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.0\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "244238b1-e3f6-4c6c-a02e-07cd9a59cd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- company_name: string (nullable = true)\n",
      " |-- company_symbol: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- tx_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Next load the dataframes from MongoDB\n",
    "df = spark.read.format(\"mongo\").load()\n",
    "#Let’s verify the data was loaded by looking at the schema:\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d4611a6-febc-49d1-aa43-86d1e1f8c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that the tx_time field is loaded as a string. We can easily convert this to a time by issuing a cast statement:\n",
    "df = df.withColumn('tx_time', df.tx_time.cast('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a36c662c-544a-4a18-88bb-47b9fee8eab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we can add a new ‘movingAverage’ column that will show a moving average based upon the previous value in the dataset. To do this we leverage the PySpark Window function as follows:\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "movAvg = df.withColumn(\"movingAverage\", F.avg(\"price\")\n",
    "             .over(Window.partitionBy(\"company_symbol\").rowsBetween(-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad831a68-0e62-47f3-8332-7dd074512bd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+-----+-------------------+------------------+\n",
      "|                 _id|        company_name|company_symbol|price|            tx_time|     movingAverage|\n",
      "+--------------------+--------------------+--------------+-----+-------------------+------------------+\n",
      "|{5f527ac22f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.38|2020-09-04 13:34:58|43.385000000000005|\n",
      "|{5f527ac32f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.39|2020-09-04 13:34:59| 43.39666666666667|\n",
      "|{5f527ac42f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.42|2020-09-04 13:35:00|43.419999999999995|\n",
      "|{5f527ac52f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.45|2020-09-04 13:35:01|43.443333333333335|\n",
      "|{5f527ac62f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.46|2020-09-04 13:35:02|             43.46|\n",
      "|{5f527ac72f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.47|2020-09-04 13:35:03| 43.47666666666667|\n",
      "|{5f527ac82f6a1552...|ITCHY ACRE CORPOR...|           IAC| 43.5|2020-09-04 13:35:04| 43.49666666666667|\n",
      "|{5f527ac92f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.52|2020-09-04 13:35:05|             43.52|\n",
      "|{5f527aca2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.54|2020-09-04 13:35:06| 43.53666666666667|\n",
      "|{5f527acb2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.55|2020-09-04 13:35:07|             43.54|\n",
      "|{5f527acc2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.53|2020-09-04 13:35:08| 43.53666666666667|\n",
      "|{5f527acd2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.53|2020-09-04 13:35:09|             43.53|\n",
      "|{5f527ace2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.53|2020-09-04 13:35:10| 43.53333333333333|\n",
      "|{5f527acf2f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.54|2020-09-04 13:35:11|43.526666666666664|\n",
      "|{5f527ad02f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.51|2020-09-04 13:35:12| 43.52333333333333|\n",
      "|{5f527ad22f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.52|2020-09-04 13:35:14| 43.51333333333333|\n",
      "|{5f527ad32f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.51|2020-09-04 13:35:15|              43.5|\n",
      "|{5f527ad42f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.47|2020-09-04 13:35:16| 43.48333333333333|\n",
      "|{5f527ad52f6a1552...|ITCHY ACRE CORPOR...|           IAC|43.47|2020-09-04 13:35:17|             43.48|\n",
      "|{5f527ad62f6a1552...|ITCHY ACRE CORPOR...|           IAC| 43.5|2020-09-04 13:35:18|             43.49|\n",
      "+--------------------+--------------------+--------------+-----+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To see our data with the new moving average column we can issue a movAvg.show().\n",
    "movAvg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a911244f-b993-404b-9e16-238ea6ba60c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update the data in our MongoDB cluster, we use the save method.\n",
    "movAvg.write.format(\"mongo\").option(\"replaceDocument\", \"true\").mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e11ab3-1e2a-4b85-aa69-94a473d48eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|                 _id|maxprice|\n",
      "+--------------------+--------+\n",
      "|FRUSTRATING CHAOS...|    87.6|\n",
      "|HOMELY KIOSK UNLI...|   86.48|\n",
      "| CREEPY GIT HOLDINGS|    83.4|\n",
      "|GREASY CHAMPION C...|   81.76|\n",
      "|COMBATIVE TOWNSHI...|   72.18|\n",
      "|FROTHY MIDNIGHT P...|   66.81|\n",
      "|ITCHY ACRE CORPOR...|   44.42|\n",
      "|LACKADAISICAL SAV...|   42.34|\n",
      "|CORNY PRACTITIONE...|   38.55|\n",
      "|TRITE JACKFRUIT P...|   22.62|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can also use the power of the MongoDB Aggregation Framework to pre-filter, sort or aggregate our MongoDB data.\n",
    "pipeline = \"[{'$group': {_id:'$company_name', 'maxprice': {$max:'$price'}}},{$sort:{'maxprice':-1}}]\"\n",
    "aggPipelineDF = spark.read.format(\"mongo\").option(\"pipeline\", pipeline).option(\"partitioner\", \"MongoSinglePartitioner\").load()\n",
    "aggPipelineDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1cc3a0e-e1ac-40bc-911a-95aa60411aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
